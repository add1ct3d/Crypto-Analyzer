{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created on:   03/03/2018  Aditya Shirode\n",
    "# Modified on:  03/08/2018  Aditya Shirode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "- Make generic functions for tasks for modularity\n",
    "- CRON jobs for all timeframes\n",
    "- One function to update all csvs\n",
    "- Plug n play for indicators\n",
    "- Add limit to queries\n",
    "- Include Active/Inactive column in CSV for active/inactive coins\n",
    "- Update_CSV_to_Latest should contain active coins from Exchanges and From CSV. Check if a (coin,exchange) tuple is active(Check if it is present on exchange using ccxt library) . If it is active   get latest data for it if already present in CSV,if not in CSV get all data. If coin is not active on exchange , we will put a Active/Inactive status in CSV accordingly. All functions will have   to be modified to run code only for active coin-exchange combinations.\n",
    "- Date Format will be '%d-%m-%Y %H:%M:%S' . This is giving me a lot of problems especially while reading data. When I don't put :%S it tells me dataframe has second and sometimes when :%S is         there, it tells me no second value in dataframe.\n",
    "- Analytics Value Accuracy. Some parameter in Jupyter.\n",
    "- Have to fetch Coins based on Parameters. Example - Fetch all active coins-exchange combinations where RSI>0 and RSI<=30. Fetch all active coins-exchange combinations where closing price is         between LOWERBAND and MIDDLEBAND. Get me intersection(common coins) of these 2 list. Now the coin from the intersection list which probably has the lowest volume can increase in price faster       then the others(Little increase in Volume will result in Big increase in Price)\n",
    "- For each active coin-exchange combination I want to check the change in Value of different Technical Indicators of 2 consecutive periods in time. Example - I want to know if for a particular       coin RSI=a on period x and RSI>a on period x+1. I want to know whenever MACD and MACD_SIGNAL cross each other(On period x MACD=a and MACD_SIGNAL=b where a<=b and on period x+1 MACD=a and           MACD_SIGNAL=b where a>b. MACD_HISTOGRAM same like RSI want to know when it is 'a' on period x and 'a++' on period x+1).\n",
    "- Convert 1D timeframe to 3D/1Week/etc. Convert 1H timeframe to 4H/6H/etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "import talib\n",
    "import logging\n",
    "import requests\n",
    "import datetime\n",
    "import importlib\n",
    "import dateutil.parser\n",
    "import ccxt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from importlib import reload\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import inspect\n",
    "import pyti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(logging)\n",
    "LOGGING_FORMAT = '%(asctime)s - %(levelname)s - %(message)s'\n",
    "logging.basicConfig(format=LOGGING_FORMAT, level=logging.\n",
    "                    INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cryptocompare_wrapper = os.path.join(os.curdir, 'cryptocompare_wrapper.py')\n",
    "cryptocompare_wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cryptocompare_wrapper as ccw\n",
    "reload(ccw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FIELDS\n",
    "PRICE = 'PRICE'\n",
    "HIGH = 'HIGH24HOUR'\n",
    "LOW = 'LOW24HOUR'\n",
    "VOLUME = 'VOLUME24HOUR'\n",
    "CHANGE = 'CHANGE24HOUR'\n",
    "CHANGE_PERCENT = 'CHANGEPCT24HOUR'\n",
    "MARKETCAP = 'MKTCAP'\n",
    "NPERIODS = 100\n",
    "TIMEFRAME = 'Day'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defaults\n",
    "CURR = 'BTC'\n",
    "EXCHANGE = 'CCCAGG'\n",
    "COIN = 'ETH'\n",
    "COIN_LIST = ['BTC', 'ETH', 'XRP']\n",
    "EXCHANGES = ['Bittrex','Binance','Kucoin','HuobiPro','Cryptopia','IDEX']\n",
    "EXCHANGES = ['Bittrex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coin DB\n",
    "#coins = ccw.get_coin_list()\n",
    "#COIN_DB = pd.DataFrame.from_dict(coins, orient='index')\n",
    "# COIN_DB.to_csv('coin_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exchange DB\n",
    "#exchanges = ccw.get_exchanges_list()\n",
    "#EXCHANGE_DB = pd.DataFrame.from_dict(exchanges, orient='index')\n",
    "#EXCHANGE_DB.to_csv('exchanges_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ccxt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-199a23c16c21>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# For every exchange, fetch it's markets. Then depending on the JSON returned, prepare a list of coins for which historical data has to be downloaded.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbittrex_exchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mccxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbittrex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mbinance_exchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mccxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mkucoin_exchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mccxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkucoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mhuobiPro_exchange\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mccxt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhuobipro\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ccxt' is not defined"
     ]
    }
   ],
   "source": [
    "# For every exchange, fetch it's markets. Then depending on the JSON returned, prepare a list of coins for which historical data has to be downloaded.\n",
    "bittrex_exchange = ccxt.bittrex()\n",
    "binance_exchange = ccxt.binance()\n",
    "kucoin_exchange = ccxt.kucoin()\n",
    "huobiPro_exchange = ccxt.huobipro()\n",
    "cryptopia_exchange = ccxt.cryptopia()\n",
    "#print(bittrex_exchange.fetchCurrencies())\n",
    "list_of_exchanges = [bittrex_exchange,binance_exchange,kucoin_exchange,huobiPro_exchange,\n",
    "                     cryptopia_exchange]\n",
    "done = False\n",
    "i=0\n",
    "#df_markets = pd.DataFrame(markets)\n",
    "#bittrex_market = bittrex_exchange.fetchMarkets()\n",
    "#binance_market = binance_exchange.fetchMarkets()\n",
    "#kucoin_market = kucoin_exchange.fetchMarkets()\n",
    "#list_of_markets = [#bittrex_market,\n",
    "                   #binance_market\n",
    " #                  kucoin_market #For kucoin the fetchMarkets function returns different dictionary keys\n",
    "  #                  ]\n",
    "coins_list = set()\n",
    "var_quote = \"\"\n",
    "\n",
    "for exchange in list_of_exchanges:\n",
    "    if exchange.name == 'Cryptopia' or exchange.name == 'Binance' or exchange.name == 'Kucoin' or exchange.name == 'Huobi Pro':\n",
    "        continue #exchange.name == 'Binance' or \n",
    "    markets = exchange.fetchMarkets()\n",
    "    for row in markets:\n",
    "        if exchange.name == 'Huobi Pro' or exchange.name == 'Cryptopia':\n",
    "            if row['base'] not in coins_list:\n",
    "                    coins_list.add(row['base'])\n",
    "            continue\n",
    "        if  'active' in row and row['active'] == True :\n",
    "            #print(exchange.name,row)\n",
    "            #sys.exit(\"Te\")\n",
    "            if exchange.name == 'Bittrex' or exchange.name == 'Binance'  :\n",
    "                var_quote = \"quoteId\"\n",
    "            elif   exchange.name == 'Kucoin' or exchange.name == 'Huobi Pro':\n",
    "                var_quote = \"quote\"\n",
    "            #print(var_quote)\n",
    "            if var_quote in row and row[var_quote] == 'BTC':\n",
    "                if row['base'] not in coins_list:\n",
    "                    coins_list.add(row['base'])\n",
    "print(list(coins_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_new_coins(csv_filename_read,csv_filename_write,timeframe):\n",
    "    csv_all_coins_full = csv_filename_read\n",
    "    csv_all_coins_full_new = csv_filename_write\n",
    "    not_updated = defaultdict(list)\n",
    "    existing_coin_exchange = []\n",
    "    # If the csv already exists, find out which coins and exchanges have already been added\n",
    "    if os.path.isfile(csv_all_coins_full):\n",
    "        df_csv_all_coins_full = pd.read_csv(csv_all_coins_full,index_col=['coin','exchange'])\n",
    "        # existing_coin_exchange is a list of tuples (coin, exchange)\n",
    "        existing_coin_exchange = np.unique(df_csv_all_coins_full.index.values)\n",
    "    \n",
    "    number_of_coins = 0\n",
    "    #existing_coin_exchange.to_csv('Existing_coin.csv')\n",
    "    \"\"\"\n",
    "    coins_list = ['NPXS', 'EVE', 'MAN', 'TOMO', 'TFD', 'BAX', 'PFR', 'EOSDAC', 'TAU', 'KIN', 'SHIP',\n",
    "                  'CPC', 'WPR', 'BDG', 'TRAC', 'CS', 'REM', 'AURA', 'DTH', 'FDZ', 'BEE', 'SEN', 'CRPT', 'TBAR', 'REN', 'POLY',\n",
    "                  'HORSE', 'XDCE', 'ELEC', 'DADI', 'INSTAR', 'CHP', 'FSN', 'ABT', 'MWAT', 'NTK', 'AGI', 'AXP', 'C20', 'XNK', 'SENC', 'HAV', 'EXRN', 'NPX',\n",
    "                  'BERRY', 'UTK', 'BLT', 'XBP', 'DRGN', 'LGO', 'RFR', 'TRX', 'GET', 'RKT', 'BBN', 'ADH', 'BANCA', 'SETH', 'CBT', 'NCASH', 'QASH', 'EVN', 'BPT', 'EOS', \n",
    "                  'SHP', 'ZIL', 'PRL', 'LALA', 'OPT', 'IDXM', 'SPHTX', 'LNC', 'ZRX', 'EMV', 'TEL', 'NCT', 'CRED', 'CXO', 'THETA', 'AION', 'ACC', 'OMG', 'LYM', 'DATX', 'COV', 'COFI',\n",
    "                  'SRN', 'STQ', 'FLUZ', 'ICX', 'LST', 'BLZ', 'ART', 'SWM', 'CPY', 'SENT', 'ADB', 'LINK', 'ETH/DAI', 'GLA', 'ARY', 'CV', 'ERC20', 'STK', 'VEN', 'CAS', 'DXT', 'EXY', '1ST',\n",
    "                  'DENT', 'HQX', 'WAX', 'HBT', 'DRG', 'STORM', 'INXT', 'KNC', 'WAND', 'QSP', 'GJC', 'PAY', 'VZT', 'BEZ', 'FOTA', 'PCL', 'DCN', 'CHSB', 'DAI', 'DMT', 'NAS', 'WABI', 'CVT', \n",
    "                  'MTN', 'BNTY', 'IDH', 'KICK', 'SIG', 'BLUE', 'REQ', 'HAT', 'PLR', 'SXUT', 'GAT', 'LEDU', 'DNT', 'REX', 'FUCK', 'VEE', 'BTO', 'SXDT', 'ALIS', 'IOST', 'CFI', 'XUC', 'VIU',\n",
    "                  'CCO', 'SAN', 'EPY', 'NIO', 'NEWB', 'CPAY', 'MCI', 'MKR', 'STU', 'CMT', 'TRDT', 'LIFE', 'EQL', 'PARETO', 'ITT', 'FYN', 'JET', 'BKX', 'NGC', 'SPF', 'HVN', 'ELIX', 'VOISE', \n",
    "                  'LEV', 'UFR', 'TKN', 'AMB', 'RHOC', 'MYST', 'J8T', 'SNT', 'TIO', 'EBET', 'PIX', 'XNN', 'STORJ', 'EVX', 'ADX', 'JNT', 'LOCI', 'MTL', 'SPANK', 'SALT', \n",
    "                  'CVC', 'ICN', 'FUN', 'DGD', 'WINGS', 'MOD', 'ENJ', 'QAU', 'ARN', 'RVT', 'LEND', 'REP', 'DTA', 'DNA', 'ESZ', 'MNTP', 'SENSE', 'AIR', 'ELF', 'ELTCOIN', 'MBRS', 'CAT', 'POWR', 'EREAL' ]\n",
    "    \"\"\"\n",
    "    #print(existing_coin_exchange)\n",
    "    #sys.exit(\"Te\")\n",
    "    for exchange in EXCHANGES :\n",
    "       for symbol in coins_list:\n",
    "         #For every symbol-exchange combination, if it is present in CSV,don't download historical Data for it.\n",
    "            combination_present = False\n",
    "            for item in existing_coin_exchange:\n",
    "                if item[0] == symbol and item[1] ==exchange:\n",
    "                    print(\"Combination Present\")\n",
    "                    combination_present = True\n",
    "                    break\n",
    "            if combination_present == True:\n",
    "                print(symbol,exchange,\"This will continue\")\n",
    "                continue\n",
    "            try:\n",
    "                # Can't fetch the same symbol in same symbol rate\n",
    "                print(symbol,exchange,\"In Try Block\")\n",
    "                func = function_period_mapping[timeframe]\n",
    "                to_curr = 'BTC'\n",
    "                if exchange == 'IDEX':\n",
    "                    to_curr = 'ETH'\n",
    "                if symbol is not to_curr:\n",
    "                    df_coin_all = func(\n",
    "                        coin=symbol,\n",
    "                        to_curr=to_curr,\n",
    "                        timestamp=time.time(),\n",
    "                        exchange=exchange\n",
    "                    )\n",
    "    \n",
    "                if df_coin_all.empty:\n",
    "                    not_updated[exchange].append(symbol)\n",
    "                    #print(symbol,exchange)\n",
    "                else:\n",
    "                    df_coin_all['exchange'] = exchange\n",
    "                    df_coin_all['coin'] = symbol\n",
    "                    #print(\"Coin Inserted\")\n",
    "                    df_coin_all = df_coin_all.reset_index().set_index(['coin','exchange', 'time'])\n",
    "                    #print(\"Index Set Again\")\n",
    "                    # If csv does not exist, write, else append\n",
    "                    if not os.path.isfile(csv_all_coins_full_new):\n",
    "                        df_coin_all.to_csv(csv_all_coins_full_new, mode='w')\n",
    "                    else:\n",
    "                        df_coin_all.to_csv(csv_all_coins_full_new, mode='a', header=False)\n",
    "                    number_of_coins = number_of_coins +1\n",
    "    \n",
    "            except Exception as e:\n",
    "                logging.error(e)\n",
    "                # logging.debug(\"Could not update data for {curr} from {exchange}\".format(curr=symbol, exchange=exchange))\n",
    "                not_updated[exchange].append(symbol)\n",
    "    \n",
    "    logging.error(\"Did not update the following. Try again.\\n {not_updated}\".format(not_updated=not_updated))\n",
    "    print(number_of_coins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_new_coins('all_coins_day_full_1day.csv','all_coins_day_full_1day_new_coins.csv','1dayfull')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_new_coins('all_coins_hour_full_1hour_.csv','all_coins_hour_full_1hour_.csv','1hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maps csv (future data objects) to period granularity\n",
    "# If we store all data together in a single data source, we'll change this to a function which returns corresponding rows\n",
    "data_csv_period_mapping = {\n",
    "    \"1day\": 'all_coins_day_full_1day.csv',\n",
    "    \"1hour\": 'all_coins_hour_full_1hour.csv',\n",
    "    \"1min\": 'all_coins_min_full_1min.csv',\n",
    "    \"1daycryptopia\":'all_coins_day_full_1day_Cryptopia.csv'\n",
    "}\n",
    "frequency_resampling_period_mapping = {\n",
    "    \"day\":'D',\n",
    "    \"hour\":'H',\n",
    "    \"min\":'M'\n",
    "}\n",
    "function_period_mapping = {\n",
    "    '1day': ccw.get_historical_price_day,\n",
    "    '1hour': ccw.get_historical_price_hour,\n",
    "    '1min': ccw.get_historical_price_minute,\n",
    "    '1dayfull' : ccw.get_historical_price_day_full,\n",
    "    '1daycryptopia' : ccw.get_historical_price_day\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_csv = pd.read_csv('all_coins_day_full.csv', index_col=None)\n",
    "indicator_list = ['unix_timestamp','BBANDS_BANDWIDTH_PERCENT','MONEY_FLOW_INDEX',\n",
    "                   'STOCH_PERCENT_K_MONEY_FLOW_INDEX','STOCH_PERCENT_D_MONEY_FLOW_INDEX','RSI','RSI_OVER_BOUGHT','RSI_OVER_SOLD',\n",
    "                   'STOCHRSI_K','STOCHRSI_D','STOCH_PERCENT_K','STOCH_PERCENT_D','STOCH_OVER_BOUGHT','STOCH_OVER_SOLD','SMA_FAST','SMA_SLOW','SMA_TEST',\n",
    "                  'MACD','MACD_SIGNAL','MACD_TEST','ON_BALANCE_VOLUME','ON_BALANCE_VOLUME_TEST']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # Technical Analysis Settings\n",
    "EMA_FAST = 10\n",
    "EMA_SLOW = 20\n",
    "RSI_PERIOD = 14\n",
    "RSI_OVER_BOUGHT = 70\n",
    "RSI_OVER_SOLD = 30\n",
    "RSI_AVG_PERIOD = 15\n",
    "MACD_FAST = 12\n",
    "MACD_SLOW = 26\n",
    "MACD_SIGNAL = 9\n",
    "STOCH_K = 14\n",
    "STOCH_D = 3\n",
    "STOCH_OVER_BOUGHT = 70\n",
    "STOCH_OVER_SOLD = 30\n",
    "from pyti import bollinger_bands\n",
    "from pyti import money_flow_index\n",
    "from pyti import stochastic\n",
    "from pyti import simple_moving_average\n",
    "from pyti import stochrsi\n",
    "from pyti import on_balance_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_indicator(csv_filename,periods,timeframe,datetimeformat_string):\n",
    "    \"\"\" Update the given csv_file with new column values for corr rows \"\"\"\n",
    "    df_csv = pd.read_csv(csv_filename, index_col=None,dayfirst=True)\n",
    "    #df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M:%S'))\n",
    "    \n",
    "    for indicator in indicator_list:\n",
    "        if indicator not in df_csv.columns and indicator not in df_csv.index:\n",
    "            df_csv[indicator] = np.nan\n",
    "    df_csv.unix_timestamp =  df_csv.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t) ,datetimeformat_string).timetuple()))\n",
    "    df_csv = df_csv.set_index(['coin', 'exchange','unix_timestamp'])\n",
    "    data = list(df_csv.index.get_level_values(0).unique())\n",
    "    i=0\n",
    "    j=0\n",
    "    for coin_name in data:\n",
    "        coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "        #print(coin_df)\n",
    "        coin_df = coin_df.reset_index()\n",
    "        coin_df = coin_df.sort_values(by=['exchange','unix_timestamp']).set_index(['coin', 'exchange','unix_timestamp'])\n",
    "        #print(coin_df)\n",
    "        df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "        for key, item in df_groupby:\n",
    "            req_data = df_groupby.get_group(key)\n",
    "            req_data2 = req_data.iloc[-periods:]\n",
    "\n",
    "            start_date = req_data2.index.get_level_values(2)[0]\n",
    "            end_date = req_data2.index.get_level_values(2)[req_data2.shape[0]-1]\n",
    "            req_data2 = req_data[(req_data.index.get_level_values(2) >= start_date) & (req_data.index.get_level_values(2) <= end_date)]\n",
    "            #print(req_data2)\n",
    "            np_volumeto = np.array(req_data2.volumeto.values,dtype='f8')\n",
    "            if len(np_volumeto)<20:\n",
    "                j = j+1\n",
    "                print(coin_name,j,\" Not Updated\")\n",
    "                continue\n",
    "            req_data2['BBANDS_BANDWIDTH_PERCENT'] = pyti.bollinger_bands.percent_b(req_data2.close.values,20)\n",
    "            req_data2['MONEY_FLOW_INDEX'] = money_flow_index.money_flow_index(req_data2.close.values, req_data2.high.values, req_data2.low.values, np_volumeto, 14)\n",
    "            req_data2['STOCH_PERCENT_K_MONEY_FLOW_INDEX'] = pyti.stochastic.percent_k(req_data2.MONEY_FLOW_INDEX.values,14) * 100\n",
    "            req_data2['STOCH_PERCENT_D_MONEY_FLOW_INDEX'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCH_PERCENT_K_MONEY_FLOW_INDEX.values,3)\n",
    "            req_data2['RSI'] = talib.RSI(req_data2.close.values, timeperiod=RSI_PERIOD)\n",
    "            req_data2['RSI_OVER_BOUGHT'] = np.where((req_data2.RSI >= RSI_OVER_BOUGHT) & (req_data2.RSI <= req_data2.RSI.shift(1)),1,0)\n",
    "            req_data2['RSI_OVER_SOLD'] = np.where((req_data2.RSI <= RSI_OVER_SOLD) & (req_data2.RSI >= req_data2.RSI.shift(1)),1,0)\n",
    "            req_data2['STOCHRSI_K'] = pyti.stochrsi.stochrsi(req_data2.close.values,14)\n",
    "            req_data2['STOCHRSI_D'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCHRSI_K.values,3)\n",
    "            req_data2['STOCH_PERCENT_K'] = pyti.stochastic.percent_k(req_data2.high.values,14) * 100\n",
    "            req_data2['STOCH_PERCENT_D'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCH_PERCENT_K.values,3)\n",
    "            req_data2['STOCH_OVER_BOUGHT'] = np.where((req_data2.STOCH_PERCENT_K >= STOCH_OVER_BOUGHT) & (req_data2.STOCH_PERCENT_K <= req_data2.STOCH_PERCENT_K.shift(1)),1,0)\n",
    "            req_data2['STOCH_OVER_SOLD'] = np.where((req_data2.STOCH_PERCENT_K <= STOCH_OVER_SOLD) & (req_data2.STOCH_PERCENT_K >= req_data2.STOCH_PERCENT_K.shift(1)),1,0)\n",
    "            req_data2['SMA_FAST'] = talib.SMA(req_data2.close.values,7)\n",
    "            req_data2['SMA_SLOW'] = talib.SMA(req_data2.close.values,21)\n",
    "            req_data2['SMA_TEST'] = np.where(req_data2.SMA_FAST>req_data2.SMA_SLOW,1,0)\n",
    "            req_data2['ON_BALANCE_VOLUME'] = on_balance_volume.on_balance_volume(req_data2.close.values,np_volumeto)\n",
    "            req_data2['ON_BALANCE_VOLUME_TEST'] = np.where(req_data2.ON_BALANCE_VOLUME>req_data2.ON_BALANCE_VOLUME.shift(1),1,0)\n",
    "            \"\"\"\n",
    "            req_data2['Accumulation_Distribution_Oscillator'] = talib.ADOSC(req_data2.high.values,req_data2.low.values\n",
    "                                                  ,req_data2.close.values,np_volumeto)\n",
    "            req_data2['ADOSC_TEST'] = np.where((req_data2.Accumulation_Distribution_Oscillator>req_data2.Accumulation_Distribution_Oscillator.shift(1)) & (req_data2.Accumulation_Distribution_Oscillator>=0) & \n",
    "                                               (req_data2.Accumulation_Distribution_Oscillator.shift(1)<=0),1,0)\n",
    "            \"\"\"\n",
    "            \n",
    "            req_data2['MACD'],req_data2['MACD_SIGNAL'],MACD_HISTOGRAM= talib.MACD(req_data2.close.values,fastperiod=MACD_FAST,slowperiod=MACD_SLOW,signalperiod=MACD_SIGNAL)\n",
    "            req_data2['MACD_TEST'] = np.where(req_data2.MACD>req_data2.MACD_SIGNAL,1,0)\n",
    "            \n",
    "            \n",
    "            df_csv.update(req_data2)\n",
    "            i = i+1\n",
    "            print(coin_name,i)\n",
    "            #print(df_csv.query('coin == @coin_name').tail(1))\n",
    "            #sys.exit(\"Testing\")\n",
    "    df_csv.to_csv(csv_filename,date_format=\"%d-%m-%Y %H:%M:%S\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_day_full_1day.csv',250,'1day','%d-%m-%Y %H:%M')\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_day_full_3days_.csv',250,'3day','%d-%m-%Y %H:%M:%S')\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_day_full_7days_.csv',250,'7day','%d-%m-%Y %H:%M:%S')\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_day_full_1day_Cryptopia.csv',250,'1day','%d-%m-%Y %H:%M')\n",
    "print(\"Done\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_day_full_3days_Cryptopia.csv',250,'3day','%d-%m-%Y %H:%M:%S')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_day_full_7days_Cryptopia.csv',250,'7day','%d-%m-%Y %H:%M:%S')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_day_full_14days_Cryptopia.csv',250,'14day','%d-%m-%Y %H:%M:%S')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_hour_full_1hour.csv',250,'1hour')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_hour_full_4hours_.csv',250,'4hour')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_hour_full_6hours_.csv',250,'6hour')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_hour_full_12hours_.csv',250,'12hour')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_indicator('all_coins_min_full_1min.csv',250,'1min')\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(csv_filename,period,resampling_multiplier,exchange,datetimeformat_string):\n",
    "    df_csv = pd.read_csv(csv_filename,dayfirst=True)\n",
    "    df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, datetimeformat_string))\n",
    "    \n",
    "    df_csv = df_csv.reset_index()\n",
    "    \n",
    "    for indicator in indicator_list:\n",
    "        if indicator not in df_csv.columns:\n",
    "            df_csv[indicator] = np.nan\n",
    "    df_csv = df_csv.set_index(['coin', 'exchange','time'])\n",
    "    data = list(df_csv.index.get_level_values(0).unique())\n",
    "    i=0\n",
    "    all_dataframes = []\n",
    "    resampling_period = \"\"+str(resampling_multiplier)+frequency_resampling_period_mapping[period]\n",
    "    output_csv_filename = \"all_coins_\"+period+\"_full_\"+str(resampling_multiplier)+period+\"s_\"+exchange+\".csv\"\n",
    "    for coin_name in data:\n",
    "        coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "        coin_df = coin_df.reset_index()\n",
    "        coin_df = coin_df.sort_values(by=['exchange','time']).set_index(['coin', 'exchange','time'])\n",
    "        #print(coin_df)\n",
    "        df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "        for key, item in df_groupby:\n",
    "            req_data = df_groupby.get_group(key)\n",
    "            req_data = req_data.resample(resampling_period,level=2,closed='right',label='right').agg({'open': 'first', \n",
    "                                     'high': 'max', \n",
    "                                     'low': 'min', \n",
    "                                     'close': 'last',\n",
    "                                    'volumeto':'sum',\n",
    "                                        'volumefrom':'sum'})\n",
    "\n",
    "            req_data['coin'] = coin_name\n",
    "            req_data['exchange'] = key\n",
    "            \n",
    "            #print(req_data)\n",
    "            req_data = req_data.reset_index()\n",
    "            req_data['unix_timestamp'] =  req_data.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t), '%Y-%m-%d %H:%M:%S').timetuple()))\n",
    "            req_data = req_data.set_index(['coin','exchange','time'])\n",
    "            i = i+1\n",
    "            print(coin_name,i)\n",
    "            all_dataframes.append(req_data)\n",
    "    pd.concat(all_dataframes).to_csv(output_csv_filename,date_format=\"%d-%m-%Y %H:%M:%S\")\n",
    "    print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1day'],'day',3,\"\",'%d-%m-%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1day'],'day',7,\"\",'%d-%m-%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daycryptopia'],'day',3,\"Cryptopia\",'%d-%m-%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daycryptopia'],'day',7,\"Cryptopia\",'%d-%m-%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1daycryptopia'],'day',14,\"Cryptopia\",'%d-%m-%Y %H:%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1hour'],'hour',4,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1hour'],'hour',6,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resample(data_csv_period_mapping['1hour'],'hour',12,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_api(coin=COIN, to_curr=CURR, nperiods=1, period='1day',exchange_name=EXCHANGES[0]):\n",
    "    \"\"\" Fetch data for coin over nperiods\n",
    "        e.g. Get data for 'BTC' for past 12 hours in hours granularity\n",
    "    \"\"\"\n",
    "    period = period.lower()\n",
    "    func = function_period_mapping[period]\n",
    "    if exchange_name == 'IDEX':\n",
    "        to_curr = 'ETH'\n",
    "    coin_last_nperiods = func(\n",
    "        coin=coin,\n",
    "        to_curr=to_curr,\n",
    "        limit=nperiods,\n",
    "        exchange=exchange_name\n",
    "    )\n",
    "    if coin_last_nperiods is not None:\n",
    "        return coin_last_nperiods.iloc[-int(nperiods):]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_csv_to_latest(period='1day'):\n",
    "    \"\"\" Update the csv for given period upto current time for coin \"\"\"\n",
    "    period = period.lower()\n",
    "    csv_filename = data_csv_period_mapping[period]  # Get corr csv\n",
    "    #csv_filename = 'Experiment.csv'\n",
    "    df_coin_period = pd.read_csv(csv_filename)  # , index_col=['coin', 'exchange']\n",
    "    csv_column_order = df_coin_period.columns.tolist()\n",
    "    df_coin_period = df_coin_period.set_index(keys=['coin', 'exchange'])\n",
    "    df_coin_period.time = df_coin_period.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M'))\n",
    "    \n",
    "    lst_new_data = []\n",
    "    PRINT_MSG = \"{:15} {!s:20} {!s:>20} {:>10}\"\n",
    "    logging.info(PRINT_MSG.format(\"Exchange\", \"Last Updated Time\", \"Elapsed Time\", \"nPeriodsAgo\"))\n",
    "    coins_in_csv = list(df_coin_period.index.get_level_values(0).unique())\n",
    "    #coins_in_csv = ['ZRX']\n",
    "    i=0\n",
    "    j=0\n",
    "    for coin in coins_in_csv:\n",
    "        df_coin_period_coin = df_coin_period.loc[coin]\n",
    "        # Group by exchange, sort on timestamp, and get the last row of that particular coin\n",
    "        last_update = df_coin_period_coin.groupby('exchange', group_keys=False).apply(lambda c: c.sort_values(by='time').tail(1))\n",
    "        logging.info(\"-\" * 10 + \" For coin - {}\".format(coin))\n",
    "        #print(last_update)\n",
    "        \n",
    "        for exchange in last_update.index.values: #For every coin exchange combination\n",
    "            last_updated_time = last_update.loc[exchange]['time'] #Get the time of the last row\n",
    "            try:\n",
    "                # elapsed_time = datetime.datetime.now() - datetime.datetime.strptime(last_updated_time, '%Y-%m-%d %H:%M:%S')\n",
    "                # elapsed_time = datetime.datetime.now() - datetime.datetime.strptime(last_updated_time, '%d-%m-%Y %H:%M')\n",
    "                elapsed_time = datetime.datetime.now() - last_updated_time\n",
    "            except ValueError as e:\n",
    "                logging.info(\"Failed to parse time {} for {}--{}\".format(last_updated_time, coin, exchange))\n",
    "                elapsed_time = datetime.datetime.now() - dateutil.parser.parse(last_updated_time)\n",
    "            nperiods_ago = elapsed_time / datetime.timedelta(days=1 if period == '1day' or period == '1daycryptopia' else 0,\n",
    "                                                             hours=1 if period == '1hour' else 0,\n",
    "                                                             minutes=1 if period == '1min' else 0,\n",
    "                                                             seconds=1)\n",
    "            nperiods_ago = np.floor(nperiods_ago)\n",
    "            \n",
    "            logging.info(PRINT_MSG.format(exchange, last_updated_time, elapsed_time, nperiods_ago))\n",
    "            \n",
    "            if nperiods_ago > 0:\n",
    "                \"\"\"\n",
    "                logging.info(\"Updating data for {coin}-{exchange} from {last_updated_time}\".format(\n",
    "                    coin=coin, exchange=exchange, last_updated_time=last_updated_time)\n",
    "                )\"\"\"\n",
    "                #sys.exit(\"Testing\")\n",
    "                new_data_coin_period = fetch_data_api(\n",
    "                    coin=coin,\n",
    "                    nperiods=nperiods_ago,\n",
    "                    period=period,\n",
    "                    exchange_name=exchange\n",
    "                )\n",
    "                #print(new_data_coin_period.shape)\n",
    "                if new_data_coin_period is None:\n",
    "                    print(coin,exchange,\" Info Not available from API\",str(j))\n",
    "                    j =j+1\n",
    "                    continue\n",
    "                new_data_coin_period['coin'] = coin\n",
    "                new_data_coin_period['exchange'] = exchange\n",
    "                new_data_coin_period = new_data_coin_period.reset_index()\n",
    "                new_data_coin_period['unix_timestamp'] =  new_data_coin_period.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t), '%d-%m-%Y %H:%M:%S').timetuple()))\n",
    "                i = i + 1\n",
    "                #print(coin,exchange,i)\n",
    "                lst_new_data.append(new_data_coin_period)\n",
    "    \n",
    "    #print(\"CSV column order \"+str(csv_column_order))\n",
    "    if lst_new_data:\n",
    "        df_new_data = pd.concat(lst_new_data)\n",
    "        df_new_data = df_new_data.reset_index()\n",
    "        curr_columns = df_new_data.columns.tolist()\n",
    "        #print(\"Current columns \"+str(curr_columns))\n",
    "        column_order = [col for col in csv_column_order if col in curr_columns]\n",
    "        #print(\"New Column Order \"+str(column_order))\n",
    "        df_new_data = df_new_data.reindex(columns=column_order)\n",
    "        df_new_data.to_csv(csv_filename, mode='a', header=False,index=False,date_format=\"%d-%m-%Y %H:%M:%S\")\n",
    "        #df_new_data.to_csv('Put it into CSV.csv')\n",
    "    print(\"Done\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_csv_to_latest('1day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_csv_to_latest('1dayCryptopia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_csv_to_latest('1hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "update_csv_to_latest('1min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell is for experimenting\n",
    "\n",
    "df_csv = pd.read_csv('Experiment.csv',dayfirst=True)\n",
    "ohlc_dict = {\n",
    "    'open':df_csv.open,\n",
    "    'close':df_csv.close,\n",
    "    'high':df_csv.high,\n",
    "    'low':df_csv.low,\n",
    "    'volumeto':df_csv.volumeto,\n",
    "    'time':df_csv.time\n",
    "}\n",
    "\n",
    "df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M:%S'))\n",
    "df_csv = df_csv.set_index(['coin', 'exchange','time'])\n",
    "#print(type(df_csv.index.get_level_values(2)))\n",
    "#df_csv = df_csv.resample('2D',level=2)\n",
    "#df_csv.to_csv(\"Re-Sampled_CSV.csv\")\n",
    "#sys.exit(\"Testing\")\n",
    "#df_csv = df_csv.reset_index()\n",
    "#df_csv = df_csv.set_index(['coin', 'exchange','time'])\n",
    "for indicator in indicator_list:\n",
    "    if indicator not in df_csv.columns:\n",
    "        df_csv[indicator] = np.nan\n",
    "data = list(df_csv.index.get_level_values(0).unique())\n",
    "#data = ['XMY']\n",
    "i=0\n",
    "for coin_name in data:\n",
    "    coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "    coin_df = coin_df.reset_index()\n",
    "    coin_df = coin_df.sort_values(by=['exchange','time']).set_index(['coin', 'exchange','time'])\n",
    "    #print(coin_df)\n",
    "    coin_df['TradingViewLink'] = \"https://www.tradingview.com/symbols/\"+coin_name+CURR\n",
    "    df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "    continue\n",
    "    for key, item in df_groupby:\n",
    "        req_data = df_groupby.get_group(key)\n",
    "        req_data2 = req_data.iloc[-250:]\n",
    "        \n",
    "        start_date = req_data2.index.get_level_values(2)[0]\n",
    "        end_date = req_data2.index.get_level_values(2)[req_data2.shape[0]-1]\n",
    "        req_data2 = req_data[(req_data.index.get_level_values(2) >= start_date) & (req_data.index.get_level_values(2) <= end_date)]\n",
    "        #print(req_data2)\n",
    "        np_volumeto = np.array(req_data2.volumeto.values,dtype='f8')\n",
    "        #req_data2['SMA_FAST'] = talib.SMA(req_data2.close.values,SMA_FAST)\n",
    "        #req_data2['SMA_SLOW'] = talib.SMA(req_data2.close.values,SMA_SLOW)\n",
    "        #req_data2['SMA_TEST'] = np.where(req_data2.SMA_FAST>req_data2.SMA_SLOW,1,0)\n",
    "        req_data2['RSI'] = talib.RSI(req_data2.close.values, timeperiod=RSI_PERIOD)\n",
    "        req_data2['RSI_OVER_BOUGHT'] = np.where((req_data2.RSI >= RSI_OVER_BOUGHT) & (req_data2.RSI < req_data2.RSI.shift(1)),1,0)\n",
    "        req_data2['RSI_OVER_SOLD'] = np.where((req_data2.RSI <= RSI_OVER_SOLD) & (req_data2.RSI > req_data2.RSI.shift(1)),1,0)\n",
    "        UPPERBANDS,MIDDLEBANDS,LOWERBANDS=  talib.BBANDS(req_data2.close.values,timeperiod=20,nbdevup=1,nbdevdn=1,matype=1)\n",
    "        req_data2['UPPERBAND'],req_data2['MIDDLEBAND'] ,req_data2['LOWERBAND']= UPPERBANDS,MIDDLEBANDS,LOWERBANDS\n",
    "        print(req_data2.index.get_level_values(0)[0],req_data2.index.get_level_values(1)[0])\n",
    "         req_data2['FIBONACCI_PIVOT_POINT'] = (req_data2.high.values+req_data2.low.values+req_data2.close.values)/3\n",
    "        req_data2['FIBONACCI_PIVOT_POINT_SUPPORT_1'] = req_data2.FIBONACCI_PIVOT_POINT.values - ((req_data2.high.values-req_data2.low.values)*0.382)\n",
    "        req_data2['FIBONACCI_PIVOT_POINT_SUPPORT_2'] = req_data2.FIBONACCI_PIVOT_POINT.values - ((req_data2.high.values-req_data2.low.values)*0.618)\n",
    "        req_data2['FIBONACCI_PIVOT_POINT_SUPPORT_3'] = req_data2.FIBONACCI_PIVOT_POINT.values - ((req_data2.high.values-req_data2.low.values)*1)\n",
    "        req_data2['FIBONACCI_PIVOT_POINT_RESISTANCE_1'] = req_data2.FIBONACCI_PIVOT_POINT.values + ((req_data2.high.values-req_data2.low.values)*0.382)\n",
    "        req_data2['FIBONACCI_PIVOT_POINT_RESISTANCE_2'] = req_data2.FIBONACCI_PIVOT_POINT.values + ((req_data2.high.values-req_data2.low.values)*0.618)\n",
    "        req_data2['FIBONACCI_PIVOT_POINT_RESISTANCE_3'] = req_data2.FIBONACCI_PIVOT_POINT.values + ((req_data2.high.values-req_data2.low.values)*1)\n",
    "        pivot_point_string = \"\"\n",
    "        if req_data2.close.values[-1] <= req_data2.FIBONACCI_PIVOT_POINT.values[-1] and req_data2.close.values[-1] >= req_data2.FIBONACCI_PIVOT_POINT_SUPPORT_1.values[-1]:\n",
    "            pivot_point_string = \"Between PP and S1\"\n",
    "        elif req_data2.close.values[-1] <= req_data2.FIBONACCI_PIVOT_POINT_SUPPORT_1.values[-1] and req_data2.close.values[-1] >= req_data2.FIBONACCI_PIVOT_POINT_SUPPORT_2.values[-1]:\n",
    "            pivot_point_string = \"Between S1 and S2\"\n",
    "        elif req_data2.close.values[-1] <= req_data2.FIBONACCI_PIVOT_POINT_SUPPORT_2.values[-1] and req_data2.close.values[-1] >= req_data2.FIBONACCI_PIVOT_POINT_SUPPORT_3.values[-1]:\n",
    "            pivot_point_string = \"Between S2 and S3\"\n",
    "        elif req_data2.close.values[-1] <= req_data2.FIBONACCI_PIVOT_POINT_SUPPORT_3.values[-1]:\n",
    "            pivot_point_string = \"Below S3\"\n",
    "        elif req_data2.close.values[-1] <= req_data2.FIBONACCI_PIVOT_POINT_RESISTANCE_1.values[-1] and req_data2.close.values[-1] >= req_data2.FIBONACCI_PIVOT_POINT.values[-1]:\n",
    "            pivot_point_string = \"Between PP and R1\"\n",
    "        elif req_data2.close.values[-1] <= req_data2.FIBONACCI_PIVOT_POINT_RESISTANCE_2.values[-1] and req_data2.close.values[-1] >= req_data2.FIBONACCI_PIVOT_POINT_RESISTANCE_1.values[-1]:\n",
    "            pivot_point_string = \"Between R1 and R2\"\n",
    "        elif req_data2.close.values[-1] <= req_data2.FIBONACCI_PIVOT_POINT_RESISTANCE_3.values[-1] and req_data2.close.values[-1] >= req_data2.FIBONACCI_PIVOT_POINT_RESISTANCE_2.values[-1]:\n",
    "            pivot_point_string = \"Between R2 and R3\"\n",
    "        elif req_data2.close.values[-1] >= req_data2.FIBONACCI_PIVOT_POINT_RESISTANCE_3.values[-1]:\n",
    "            pivot_point_string = \"Above R3\"\n",
    "        req_data2['CLOSING_PRICE_PIVOT_POINT_COMPARISON'] = pivot_point_string\n",
    "        print(req_data2['CLOSING_PRICE_PIVOT_POINT_COMPARISON'])\n",
    "        continue\n",
    "        req_data2['Accumulation_Distribution_Line'] = talib.AD(req_data2.high.values,req_data2.low.values\n",
    "                                              ,req_data2.close.values,np_volumeto)\n",
    "        req_data2['MACD'],req_data2['MACD_SIGNAL'],req_data2['MACD_HISTOGRAM'] = talib.MACD(req_data2.close.values,fastperiod=MACD_FAST,slowperiod=MACD_SLOW,signalperiod=MACD_SIGNAL)\n",
    "        req_data2['MACD_TEST'] = np.where(req_data2.MACD>req_data2.MACD_SIGNAL,1,0)\n",
    "        \n",
    "        req_data2['STOCH_RSI_FASTK'],req_data2['STOCH_RSI_FASTD'] = talib.STOCHRSI(req_data2.close.values,fastk_period=14)\n",
    "        print(req_data2['STOCH_RSI_FASTK'],req_data2['STOCH_RSI_FASTD']) \n",
    "        sys.exit(\"Testing\")\n",
    "        \n",
    "        req_data2['STOCH_K'],req_data2['STOCH_D'] = talib.STOCH(req_data2.high.values,req_data2.low.values,req_data2.close.values, slowk_period=STOCH_K,slowd_period=STOCH_D)\n",
    "        req_data2['STOCH_OVER_BOUGHT'] = np.where((req_data2.STOCH_K >= STOCH_OVER_BOUGHT) & (req_data2.STOCH_K < req_data2.STOCH_K.shift(1)),1,0)\n",
    "        req_data2['STOCH_OVER_SOLD'] = np.where((req_data2.STOCH_K <= STOCH_OVER_SOLD) & (req_data2.STOCH_K > req_data2.STOCH_K.shift(1)),1,0)\n",
    "        #print(req_data2['STOCH_OVER_BOUGHT'],req_data2['STOCH_OVER_SOLD'])\n",
    "        #\n",
    "        df_csv.update(req_data2)\n",
    "        i = i+1\n",
    "        print(coin_name,i)\n",
    "        #print(req_data2.tail(10))\n",
    "        #sys.exit(\"Testing\")\n",
    "df_csv.to_csv('Experiment.csv',date_format=\"%d-%m-%Y %H:%M:%S\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(talib.MA_Type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(talib.SMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyti.stochastic import percent_k\n",
    "help(percent_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_csv = pd.read_csv('all_coins_day_full.csv', index_col=None)\n",
    "indicator_list2 = ['BBANDS_BANDWIDTH_PERCENT','MONEY_FLOW_INDEX',\n",
    "                   'STOCH_PERCENT_K_MONEY_FLOW_INDEX','STOCH_PERCENT_D_MONEY_FLOW_INDEX',\n",
    "                   'STOCHRSI_K','STOCHRSI_D','STOCH_PERCENT_K','STOCH_PERCENT_D','SMA_FAST','SMA_SLOW','SMA_TEST']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('Experiment.csv', index_col=None,dayfirst=True)\n",
    "df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M'))\n",
    "df_csv = df_csv.set_index(['coin', 'exchange','time'])\n",
    "for indicator in indicator_list2:\n",
    "    if indicator not in df_csv.columns:\n",
    "        df_csv[indicator] = np.nan\n",
    "data = list(df_csv.index.get_level_values(0).unique())\n",
    "i=0\n",
    "for coin_name in data:\n",
    "    coin_df = df_csv[df_csv.index.get_level_values(0)==coin_name] \n",
    "    coin_df = coin_df.reset_index()\n",
    "    coin_df = coin_df.sort_values(by=['exchange','time']).set_index(['coin', 'exchange','time'])\n",
    "    #print(coin_df)\n",
    "    df_groupby = coin_df.groupby(['exchange'], group_keys=False)\n",
    "    for key, item in df_groupby:\n",
    "        req_data = df_groupby.get_group(key)\n",
    "        req_data2 = req_data.iloc[-250:]\n",
    "\n",
    "        start_date = req_data2.index.get_level_values(2)[0]\n",
    "        end_date = req_data2.index.get_level_values(2)[req_data2.shape[0]-1]\n",
    "        req_data2 = req_data[(req_data.index.get_level_values(2) >= start_date) & (req_data.index.get_level_values(2) <= end_date)]\n",
    "        #print(req_data2)\n",
    "        np_volumeto = np.array(req_data2.volumeto.values,dtype='f8')\n",
    "        if len(np_volumeto)<20:\n",
    "            continue\n",
    "        i = i+1\n",
    "        print(coin_name,i)\n",
    "        req_data2['BBANDS_BANDWIDTH_PERCENT'] = pyti.bollinger_bands.percent_b(req_data2.close.values,20)\n",
    "        req_data2['MONEY_FLOW_INDEX'] = money_flow_index(req_data2.close.values, req_data2.high.values, req_data2.low.values, np_volumeto, 14)\n",
    "        req_data2['STOCH_PERCENT_K_MONEY_FLOW_INDEX'] = pyti.stochastic.percent_k(req_data2.MONEY_FLOW_INDEX.values,14)\n",
    "        req_data2['STOCH_PERCENT_D_MONEY_FLOW_INDEX'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCH_PERCENT_K_MONEY_FLOW_INDEX.values,3)\n",
    "        req_data2['STOCHRSI_K'] = pyti.stochrsi.stochrsi(req_data2.close.values,14)\n",
    "        req_data2['STOCHRSI_D'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCHRSI_K.values,3)\n",
    "        req_data2['STOCH_PERCENT_K'] = pyti.stochastic.percent_k(req_data2.high.values,14)\n",
    "        req_data2['STOCH_PERCENT_D'] = pyti.simple_moving_average.simple_moving_average(req_data2.STOCH_PERCENT_K.values,3)\n",
    "        req_data2['SMA_FAST'] = talib.SMA(req_data2.close.values,50)\n",
    "        req_data2['SMA_SLOW'] = talib.SMA(req_data2.close.values,200)\n",
    "        req_data2['SMA_TEST'] = np.where(req_data2.SMA_FAST>req_data2.SMA_SLOW,1,0)\n",
    "        #print(req_data2['UPPERBANDS'])\n",
    "        df_csv.update(req_data2)\n",
    "        #print(df_csv.query('coin == @coin_name').tail(1))\n",
    "df_csv.to_csv('Experiment.csv',date_format=\"%d-%m-%Y %H:%M:%S\")\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator_list3 = ['unix_timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = pd.read_csv('all_coins_day_full_1day_Cryptopia.csv', index_col=None,dayfirst=True)\n",
    "for indicator in indicator_list3:\n",
    "    if indicator not in df_csv.columns:\n",
    "        df_csv[indicator] = np.nan\n",
    "#df_csv.time = df_csv.time.apply(lambda t: datetime.datetime.strptime(t, '%d-%m-%Y %H:%M'))\n",
    "df_csv.unix_timestamp =  df_csv.time.apply(lambda t: time.mktime(datetime.datetime.strptime(str(t), '%d-%m-%Y %H:%M').timetuple()))\n",
    "df_csv = df_csv.set_index(['coin', 'exchange','unix_timestamp'])\n",
    "df_csv.to_csv('all_coins_day_full_1day_Cryptopia.csv',date_format=\"%d-%m-%Y %H:%M:%S\")\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
